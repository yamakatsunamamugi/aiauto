#!/usr/bin/env python3
"""
Ollamaçµ±åˆã‚·ã‚¹ãƒ†ãƒ  ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ†ã‚¹ãƒˆ
ç°¡å˜ã«Ollamaã§AIå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆã§ãã¾ã™
"""

from src.automation.ollama_handler import OllamaAIHandler
from src.automation.ollama_config import OllamaConfig

def main():
    print("ğŸ¤– Ollama AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ  ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    try:
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼åˆæœŸåŒ–
        print("ğŸ”„ Ollamaãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–ä¸­...")
        handler = OllamaAIHandler()
        print("âœ… åˆæœŸåŒ–å®Œäº†ï¼")
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’è¡¨ç¤º
        print(f"\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {handler.available_models}")
        
        # è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
        config = OllamaConfig.get_default_config()
        print(f"\nâš™ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š:")
        print(f"   - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: {config['default_model']}")
        print(f"   - ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {list(config['system_prompts'].keys())}")
        
        # æ¨å¥¨è¨­å®šã‚’è¡¨ç¤º
        recommendations = OllamaConfig.get_model_recommendations()
        print(f"\nğŸ¯ ç”¨é€”åˆ¥æ¨å¥¨è¨­å®š:")
        for purpose, settings in recommendations.items():
            print(f"   - {purpose}: {settings['model']} ({settings['description']})")
        
        print("\n" + "=" * 50)
        print("ğŸ’¬ å¯¾è©±ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™ã€‚'quit'ã§çµ‚äº†")
        print("=" * 50)
        
        while True:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            user_input = input("\nè³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'çµ‚äº†', 'q']:
                break
                
            if not user_input:
                continue
            
            print("ğŸ”„ å‡¦ç†ä¸­...")
            
            # AIå‡¦ç†å®Ÿè¡Œ
            result = handler.process_text(
                text=user_input,
                model="llama3.1:8b",
                system_prompt="è¦ªåˆ‡ã§ä¸å¯§ãªæ—¥æœ¬èªã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            )
            
            if result["success"]:
                print(f"ğŸ¤– å›ç­”: {result['result']}")
                print(f"â±ï¸ å‡¦ç†æ™‚é–“: {result['processing_time']:.2f}ç§’")
            else:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        
        # æœ€çµ‚çµ±è¨ˆ
        stats = handler.get_statistics()
        print(f"\nğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
        print(f"   - ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats['total_requests']}")
        print(f"   - æˆåŠŸ: {stats['successful_requests']}")
        print(f"   - å¤±æ•—: {stats['failed_requests']}")
        print(f"   - å¹³å‡å‡¦ç†æ™‚é–“: {stats['average_response_time']:.2f}ç§’")
        
        print("\nğŸ‘‹ ãƒ†ã‚¹ãƒˆã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nğŸ“‹ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print("1. OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª: ollama --version")
        print("2. Ollamaã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª: ollama list")
        print("3. llama3.1:8bãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª")

if __name__ == "__main__":
    main()